{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OpT-VhBVMjQxdW8kmIhZtGITKGv87sFQ","authorship_tag":"ABX9TyMls5oxJdQja00P3lUyEVK1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from collections import defaultdict\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","import copy\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import math"],"metadata":{"id":"ysnmPEP7le_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file = files.upload()"],"metadata":{"id":"Yj3wYns_8ylh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"lN87L8TjIlQQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708014566054,"user_tz":-330,"elapsed":73877,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"2e431d0b-13d1-46b7-bd17-92eadd2362cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","with open('/content/drive/MyDrive/Colab Notebooks/Brown_train.txt','r') as file:\n","  data = file.readlines()\n","  # data = file.read()"],"metadata":{"id":"SQDD0mMGPedo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# tokens = nltk.word_tokenize(data)\n","# print(tokens[:3])\n","\n","tokens = []\n","for i in data:\n","  tokens.append([tuple(j.split(\"/\")) for j in i.split()])\n","# for i in data:\n","#   tokens.append([j for j in i.split()])\n","  # tokens.extend(i.split())\n","\n","l1 = tokens\n","for i in l1:\n","  for j in i:\n","    if(len(j)>2):\n","      i.append((j[0],j[-1]))\n","      i.remove(j)\n","\n","tokens = l1\n","# l1=[]\n","# for i in tokens:\n","#   for j in i:\n","#     if(len(j)>2):\n","#       l1.append(j)\n","# print(l1)\n","print(tokens[:3])\n","\n","from sklearn.model_selection import train_test_split\n","# split data into training and test set in the ratio 80:20\n","train_set,test_set = train_test_split(tokens,train_size=0.80,test_size=0.20,random_state = None)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_7y9SDpLt_4","executionInfo":{"status":"ok","timestamp":1708017596321,"user_tz":-330,"elapsed":828,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"4607a9fa-18c5-4e91-fe8d-aaac7f4caa9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[('At', 'ADP'), ('that', 'DET'), ('time', 'NOUN'), ('highway', 'NOUN'), ('engineers', 'NOUN'), ('traveled', 'VERB'), ('rough', 'ADJ'), ('and', 'CONJ'), ('dirty', 'ADJ'), ('roads', 'NOUN'), ('to', 'PRT'), ('accomplish', 'VERB'), ('their', 'DET'), ('duties', 'NOUN'), ('.', '.')], [('Using', 'VERB'), ('privately-owned', 'ADJ'), ('vehicles', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('personal', 'ADJ'), ('hardship', 'NOUN'), ('for', 'ADP'), ('such', 'ADJ'), ('employees', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('the', 'DET'), ('matter', 'NOUN'), ('of', 'ADP'), ('providing', 'VERB'), ('state', 'NOUN'), ('transportation', 'NOUN'), ('was', 'VERB'), ('felt', 'VERB'), ('perfectly', 'ADV'), ('justifiable', 'ADJ'), ('.', '.')], [('Once', 'ADP'), ('the', 'DET'), ('principle', 'NOUN'), ('was', 'VERB'), ('established', 'VERB'), (',', '.'), ('the', 'DET'), ('increase', 'NOUN'), ('in', 'ADP'), ('state-owned', 'ADJ'), ('vehicles', 'NOUN'), ('came', 'VERB'), ('rapidly', 'ADV'), ('.', '.')]]\n"]}]},{"cell_type":"code","source":["tagged_data=[]\n","for i in train_set:\n","  tagged_data.extend(i)\n","print(tagged_data[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6CedWqEvlKx","executionInfo":{"status":"ok","timestamp":1708017694151,"user_tz":-330,"elapsed":607,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"aadf900e-221b-4f0b-9313-0816057f7c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('To', 'ADP'), ('his', 'DET'), ('puzzlement', 'NOUN')]\n"]}]},{"cell_type":"code","source":["class HMM:\n","    def __init__(self, order):\n","        self.order = order\n","        self.states = set()\n","        self.start_prob = defaultdict(float)\n","        self.transition_prob = defaultdict(lambda: defaultdict(float))\n","        self.emission_prob = defaultdict(lambda: defaultdict(float))\n","\n","    def train(self, dataset):\n","        tag_count = defaultdict(int)\n","        tag_pair_count = defaultdict(lambda: defaultdict(int))\n","        tag_triple_count = defaultdict(lambda: defaultdict(int))\n","        tag_word_count = defaultdict(lambda: defaultdict(int))\n","\n","        for sentence in dataset:\n","            prev_tags = ['<s>'] * self.order\n","            for word, tag in sentence:\n","                tag_count[tag] += 1\n","                for i in range(1, self.order + 1):\n","                    if i == 1:\n","                        tag_pair_count[prev_tags[-1]][tag] += 1\n","                    else:\n","                        tag_triple_count[tuple(prev_tags[-i:])][tag] += 1\n","                tag_word_count[tag][word] += 1\n","                self.states.add(tag)\n","                prev_tags.append(tag)\n","                prev_tags.pop(0)\n","\n","        total_tags = sum(tag_count.values())\n","\n","        for tag, count in tag_count.items():\n","            self.start_prob[tag] = count / total_tags\n","\n","        for prev_tag, next_tags in tag_pair_count.items():\n","            total_transitions = sum(next_tags.values())\n","            for next_tag, count in next_tags.items():\n","                self.transition_prob[prev_tag][next_tag] = count / total_transitions\n","\n","        for prev_tags, next_tags in tag_triple_count.items():\n","            total_transitions = sum(next_tags.values())\n","            for next_tag, count in next_tags.items():\n","                self.transition_prob[prev_tags][next_tag] = count / total_transitions\n","\n","        for tag, word_counts in tag_word_count.items():\n","            total_tag = tag_count[tag]\n","            for word, count in word_counts.items():\n","                self.emission_prob[tag][word] = count / total_tag\n","\n","    def tag(self, sequence):\n","        V = [{}]\n","        path = {}\n","\n","        for state in self.states:\n","            if sequence[0] in self.emission_prob[state]:\n","                V[0][state] = self.start_prob[state] * self.emission_prob[state][sequence[0]]\n","            else:\n","                V[0][state] = 0\n","            path[state] = [state]\n","\n","        for t in range(1, len(sequence)):\n","            V.append({})\n","            new_path = {}\n","\n","            for state in self.states:\n","                if sequence[t] in self.emission_prob[state]:\n","                    (prob, prev_state) = max((V[t - 1][prev_state] * self.transition_prob[prev_state][state] * self.emission_prob[state][sequence[t]], prev_state) for prev_state in self.states)\n","                    V[t][state] = prob\n","                    new_path[state] = path[prev_state] + [state]\n","                else:\n","                    V[t][state] = 0\n","            path = new_path\n","\n","        (prob, state) = max((V[len(sequence) - 1][final_state], final_state) for final_state in self.states)\n","        return path[state]\n","\n","\n","    def viterbi(self, observations):\n","        # Initialize variables\n","        viterbi = [{state: {\"prob\": 0, \"prev_state\": None} for state in self.states} for _ in range(len(observations))]\n","        # Initialize starting probabilities\n","        for state in self.states:\n","            viterbi[0][state][\"prob\"] = self.start_prob[state] * self.emission_prob[state][observations[0]]\n","            viterbi[0][state][\"prev_state\"] = None\n","\n","        # Forward pass\n","        for t in range(1, len(observations)):\n","            for state in self.states:\n","                max_prob = 0\n","                max_prev_state = None\n","                for prev_state in self.states:\n","                    prob = viterbi[t-1][prev_state][\"prob\"] * self.transition_prob[prev_state][state] * self.emission_prob[state][observations[t]]\n","                    if prob > max_prob:\n","                        max_prob = prob\n","                        max_prev_state = prev_state\n","                viterbi[t][state][\"prob\"] = max_prob\n","                viterbi[t][state][\"prev_state\"] = max_prev_state\n","\n","        # Backtrack to find the best path\n","        best_path = []\n","        max_prob = 0\n","        max_state = None\n","        for state in self.states:\n","            if viterbi[-1][state][\"prob\"] > max_prob:\n","                max_prob = viterbi[-1][state][\"prob\"]\n","                max_state = state\n","        best_path.append(max_state)\n","        prev_state = max_state\n","        for t in range(len(observations)-1, 0, -1):\n","            prev_state = viterbi[t][prev_state][\"prev_state\"]\n","            best_path.insert(0, prev_state)\n","\n","        return best_path"],"metadata":{"id":"AH92cKzbuNUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hmm_bi = HMM(order=1)\n","hmm_tri = HMM(order=2)\n","\n","hmm_bi.train(tokens)\n","hmm_tri.train(tokens)\n","\n"],"metadata":{"id":"o1-GSrgGufMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(hmm_bi.states)\n","print(hmm_bi.start_prob)\n","print(hmm_bi.emission_prob)\n","print(hmm_bi.transition_prob)\n","print(\"\\n\")\n","# print(\"\\n\")\n","print(\"\\n\")\n","print(hmm_tri.states)\n","print(hmm_tri.start_prob)\n","print(hmm_tri.emission_prob)\n","print(hmm_tri.transition_prob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1kkDR1Sy1R-MWxXR71mG3DtswFqPyevDU"},"id":"6YlWb6I7zKwh","executionInfo":{"status":"ok","timestamp":1708020220479,"user_tz":-330,"elapsed":2244,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"fabd8366-065c-45f2-cdb5-343f4cced0ed"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["print(test_set[0])\n","\n","for i in test_set:\n","  sentence = []\n","  tags = []\n","  for j in i:\n","    sentence.append(j[0])\n","    tags.append(j[1])\n","  # print(hmm_bi.viterbi(sentence))\n","  # print(hmm_tri.viterbi(sentence))\n","\n","# sentence = []\n","# tags = []\n","# for i in test_set[0]:\n","#   # for j in i:\n","#   sentence.append(i[0])\n","#   tags.append(i[1])\n","\n","# print(hmm_bi.viterbi(sentence))\n","# print(hmm_tri.viterbi(sentence))\n","\n","# print(tags)\n","\n","  y_test_fold = hmm_bi.viterbi(sentence)\n","  y_pred_fold = tags\n","\n","  accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)\n","  precision_fold = precision_score(y_test_fold, y_pred_fold, average='weighted')\n","  recall_fold = recall_score(y_test_fold, y_pred_fold, average='weighted')\n","  f1_fold = f1_score(y_test_fold, y_pred_fold, average='weighted')\n","\n","  print(f1_fold)"],"metadata":{"id":"d3-WEL5m4EKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WzFboiGEAwWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import numpy as np\n","\n","# Assuming you have a model named 'your_model' and data 'X_train', 'y_train'\n","# X_train is your feature matrix, and y_train is your target variable\n","\n","X_train = train_set\n","y_train = test_set\n","\n","# Initialize metrics lists for each fold\n","accuracy_list = []\n","precision_list = []\n","recall_list = []\n","f1_list = []\n","\n","# Initialize StratifiedKFold with 5 folds\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Iterate through each fold\n","for train_index, test_index in kf.split(X_train, y_train):\n","    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n","    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","\n","    # Assuming 'your_model' is a classifier, you need to train it on the training fold\n","    your_model.fit(X_train_fold, y_train_fold)\n","\n","    # Predict on the test fold\n","    y_pred_fold = your_model.predict(X_test_fold)\n","\n","    # Calculate metrics for this fold\n","    accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)\n","    precision_fold = precision_score(y_test_fold, y_pred_fold, average='weighted')\n","    recall_fold = recall_score(y_test_fold, y_pred_fold, average='weighted')\n","    f1_fold = f1_score(y_test_fold, y_pred_fold, average='weighted')\n","\n","    # Append metrics to lists\n","    accuracy_list.append(accuracy_fold)\n","    precision_list.append(precision_fold)\n","    recall_list.append(recall_fold)\n","    f1_list.append(f1_fold)\n","\n","# Calculate average metrics across all folds\n","average_accuracy = np.mean(accuracy_list)\n","average_precision = np.mean(precision_list)\n","average_recall = np.mean(recall_list)\n","average_f1 = np.mean(f1_list)\n","\n","# Print individual fold results\n","for i in range(5):\n","    print(f\"Fold {i + 1}: Accuracy={accuracy_list[i]}, Precision={precision_list[i]}, Recall={recall_list[i]}, F1-Score={f1_list[i]}\")\n","\n","# Print average results\n","print(\"\\nAverage Results:\")\n","print(f\"Average Accuracy: {average_accuracy}\")\n","print(f\"Average Precision: {average_precision}\")\n","print(f\"Average Recall: {average_recall}\")\n","print(f\"Average F1-Score: {average_f1}\")"],"metadata":{"id":"VEzZAHJ0PdZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# *RNN*"],"metadata":{"id":"VB4HGaXn_Hgo"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed"],"metadata":{"id":"81neBk2jDUrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_data(file_path):\n","    sentences = []\n","    tags = []\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            line = line.strip().split()\n","            sentence = []\n","            tag = []\n","            for word_tag_pair in line:\n","                # print(word_tag_pair)\n","                if len(word_tag_pair.split('/'))== 2:\n","                  word, tag_label = word_tag_pair.split('/')\n","                  sentence.append(word)\n","                  tag.append(tag_label)\n","            sentences.append(sentence)\n","            tags.append(tag)\n","    return sentences, tags\n","\n","file_path = '/content/Brown_train.txt'\n","sentences, tags = read_data(file_path)"],"metadata":{"id":"oxBLXfV3DXMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnn_dataset = []\n","for i,j in list(zip(sentences,tags)):\n","  temp = (\" \".join(i),j)\n","  rnn_dataset.append(temp)"],"metadata":{"id":"66aBy7MQDY-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"w30cMrchDbXF"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# Hyperparameters\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 50\n","LEARNING_RATE = 0.01\n","NUM_EPOCHS = 5\n","\n","training_data = rnn_dataset\n","# Create vocabulary and POS tag sets\n","word_to_ix = {}\n","tag_to_ix = {}\n","for sentence, tags in training_data:\n","    for word in sentence.split():\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)\n","    for tag in tags:\n","        if tag not in tag_to_ix:\n","            tag_to_ix[tag] = len(tag_to_ix)\n","\n","# Convert training data to numerical format\n","X_train = []\n","y_train = []\n","for sentence, tags in training_data:\n","    X_train.append([word_to_ix[word] for word in sentence.split()])\n","    y_train.append([tag_to_ix[tag] for tag in tags])\n","\n","# Pad sequences to make them the same length\n","MAX_LEN = max(len(sentence) for sentence in X_train)\n","X_train = pad_sequences(X_train, maxlen=MAX_LEN, padding='post')\n","y_train = pad_sequences(y_train, maxlen=MAX_LEN, padding='post')\n","\n","# Model definition\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(len(word_to_ix), EMBEDDING_DIM),\n","    tf.keras.layers.SimpleRNN(HIDDEN_DIM, return_sequences=True),\n","    tf.keras.layers.Dense(len(tag_to_ix), activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=NUM_EPOCHS)"],"metadata":{"id":"3Jut63V0DbnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example inference\n","sentence = \"At that time highway engineers traveled rough and dirty roads to accomplish their duties\"\n","input_sentence = [word_to_ix[word] for word in sentence.split()]\n","input_sentence = pad_sequences([input_sentence], maxlen=MAX_LEN, padding='post')\n","predicted_tags = model.predict(input_sentence)\n","predicted_tags = [np.argmax(tag) for tag in predicted_tags[0]]\n","predicted_tags = [list(tag_to_ix.keys())[list(tag_to_ix.values()).index(idx)] for idx in predicted_tags]\n","print(\"Predicted POS tags:\", predicted_tags[:len(sentence.split())])"],"metadata":{"id":"aIz9CWftDd8E"},"execution_count":null,"outputs":[]}]}