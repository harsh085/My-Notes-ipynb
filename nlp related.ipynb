{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjEptQsuz83Er3rspsDpVL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import string\n","import nltk\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","from statistics import mean\n","from collections import Counter\n","from nltk import ngrams\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from copy import deepcopy\n","import operator\n","from math import log2\n","import urllib.request\n","\n","# Download required modules\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Train/Test data url\n","train_url = 'http://cogcomp.org/Data/QA/QC/train_5500.label'\n","test_url = 'http://cogcomp.org/Data/QA/QC/TREC_10.label'\n","\n","# Returns n-grams\n","def get_ngrams(data, n):\n","    tokens = [token for token in data.split(\" \") if token != \"\"]\n","    res = list(ngrams(tokens, n))\n","    return res\n","\n","# Get the set of English stop words\n","stop_words = set(stopwords.words('english'))\n","\n","def get_pos_tag(text):\n","    tokenized = sent_tokenize(text)\n","    wordsList = nltk.word_tokenize(tokenized[0])\n","    wordsList = [w for w in wordsList if not w in stop_words]\n","    tagged = nltk.pos_tag(wordsList)\n","    return tagged\n","\n","def build_data(path):\n","    data = []\n","    uni = []\n","    bi = []\n","    tri = []\n","    pos = []\n","    response = urllib.request.urlopen(path)\n","    file = response.read().decode('ISO-8859-1').split('\\n')\n","    # file = open(\"/content/traindata.txt\", encoding = \"ISO-8859-1\")\n","    # print(file)\n","    for line in file:\n","        # print(line)\n","        if line==\" \" or line==\"\":\n","          break\n","        line = line.split(':')\n","        row = []\n","        row.append(line[0])\n","        row.append(' '.join(line[1].split(' ')[1:]).translate(str.maketrans('', '', string.punctuation)).rstrip())\n","        length = len(row[1].split(' '))\n","        unigram = get_ngrams(row[1], 1)\n","        bigram = get_ngrams(row[1], 2)\n","        trigram = get_ngrams(row[1], 3)\n","        postag = get_pos_tag(row[1])\n","        row.append(length)\n","        row.append(unigram)\n","        uni.extend(unigram)\n","        row.append(bigram)\n","        bi.extend(bigram)\n","        row.append(trigram)\n","        tri.extend(trigram)\n","        row.append(postag)\n","        pos.extend(postag)\n","        data.append(row)\n","    # print(data, uni, bi, tri, pos)\n","    return data, uni, bi, tri, pos\n","\n","data, uni, bi, tri, pos = build_data(test_url)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGT_IJqRabmf","executionInfo":{"status":"ok","timestamp":1694930455919,"user_tz":-330,"elapsed":1294,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"1c4831c5-2649-4e50-e086-3611cc63af6d"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["\n","def frequent_grams(g, top_n):\n","    return Counter(g).most_common(top_n)\n","\n","unigram_counts = frequent_grams(uni, 500)\n","bigram_counts = frequent_grams(bi, 300)\n","trigram_counts = frequent_grams(tri, 200)\n","pos_counts = frequent_grams(pos, 500)\n","avgLength = mean([row[2] for row in data])\n","print(avgLength)\n","\n","def is_numeric(val):\n","    return isinstance(val, int) or isinstance(val, float)\n","\n","\n","header = ['Label', 'Text', 'Length', 'Unigram', 'Bigram', 'Trigram']\n","\n","class Feature:\n","    def __init__(self, col, val):\n","        self.column = col\n","        self.value = val\n","\n","    def match(self, ex):\n","        val = ex[self.column]\n","\n","        if is_numeric(val):\n","            return val <= self.value\n","        return self.value in val\n","\n","    def __repr__(self):\n","        condition = \"exists\"\n","        return \"Does %s %s %s?\" % (\n","            header[self.column], str(self.value), condition)\n","\n","def class_fref(rows):\n","    counts = {}\n","    for row in rows:\n","        label = row[0]\n","        if label not in counts:\n","            counts[label] = 0\n","        counts[label] += 1\n","    return counts\n","\n","\n","def gini(rows):\n","    counts = class_fref(rows)\n","    imp = 1\n","    for label in counts:\n","        prob_of_label = counts[label] / float(len(rows))\n","        imp -= prob_of_label**2\n","    return imp\n","\n","\n","def misclassifcation_error(rows):\n","    counts = class_fref(rows)\n","    max_prob = 0\n","    for label in counts:\n","        prob_of_label = counts[label] / float(len(rows))\n","        if prob_of_label > max_prob:\n","            max_prob = prob_of_label\n","    return 1 - max_prob\n","\n","\n","def entropy(rows):\n","    counts = class_fref(rows)\n","    imp = 0\n","    for label in counts:\n","        prob_of_label = counts[label] / float(len(rows))\n","        imp -= prob_of_label*log2(prob_of_label)\n","    return imp\n","\n","def info_gain(left, right, curr_uncertainty, func):\n","    p = float(len(left)) / (len(left) + len(right))\n","    return curr_uncertainty - p * func(left) - (1 - p) * func(right)\n","\n","class Leaf:\n","    def __init__(self, r):\n","        self.predictions = class_fref(r)\n","\n","class Decision_Node:\n","    def __init__(self,\n","                 feat,\n","                 true_b,\n","                 false_b):\n","        self.Feature = feat\n","        self.true_branch = true_b\n","        self.false_branch = false_b\n","\n","Features = []\n","\n","for y in unigram_counts:\n","    Features.append(Feature(3, y[0]))\n","\n","for y in bigram_counts:\n","    Features.append(Feature(4, y[0]))\n","\n","for y in trigram_counts:\n","    Features.append(Feature(5, y[0]))\n","\n","for y in pos_counts:\n","    Features.append(Feature(6, y[0]))\n","\n","Features.append(Feature(2, avgLength))\n","\n","print(len(Features))\n","# print(Features[1500])\n","\n","def partition(rows, Feature):\n","    trueRows = []\n","    falseRows = []\n","\n","    for row in rows:\n","        if Feature.match(row):\n","            trueRows.append(row)\n","        else:\n","            falseRows.append(row)\n","    return trueRows, falseRows\n","\n","def findBestSplit(rows, Features, func):\n","    best_gain = 0\n","    best_Feature = None\n","    current_uncertainty = func(rows)\n","\n","    for f in Features:\n","        trueRows, falseRows = partition(rows, f)\n","        if len(trueRows) == 0 or len(falseRows) == 0:\n","            continue\n","\n","        gain = info_gain(trueRows, falseRows, current_uncertainty, func)\n","\n","        if gain >= best_gain:\n","            best_gain, best_Feature = gain, f\n","\n","    return best_gain, best_Feature\n","\n","def formTree(rows, Features, func):\n","    gain, Feature = findBestSplit(rows, Features, func)\n","\n","    if gain == 0:\n","        return Leaf(rows)\n","\n","    trueRows, falseRows = partition(rows, Feature)\n","    Features.remove(Feature)\n","\n","    trueBranch = formTree(trueRows, Features, func)\n","    falseBranch = formTree(falseRows, Features, func)\n","\n","    return Decision_Node(Feature, trueBranch, falseBranch)\n","\n","def classifyRow(node, row):\n","    if isinstance(node, Leaf):\n","        return node.predictions\n","\n","    if node.Feature.match(row):\n","        return classifyRow(node.true_branch, row)\n","    else:\n","        return classifyRow(node.false_branch, row)\n","\n","def train(data, Features, func):\n","    return formTree(data, deepcopy(Features), func)\n","\n","def classify(root, rows):\n","    predictions = []\n","    for r in rows:\n","        predictions.append(max(classifyRow(root, r).items(), key=operator.itemgetter(1))[0])\n","    return predictions\n","\n","def getDataInIndex(data, index):\n","    l = []\n","    for i in range(len(data)):\n","        if i in index:\n","            l.append(data[i])\n","    return l\n","\n","\n","def getActualLabels(act_data):\n","    act_labels = []\n","    for d in act_data:\n","        act_labels.append(d[0])\n","    return act_labels\n","\n","kfold = KFold(10)\n","precision = []\n","recall = []\n","f_score = []\n","i = 0\n","\n","for trainInd,testInd in kfold.split(data):\n","    train_data = getDataInIndex(data, trainInd)\n","    test_data = getDataInIndex(data, testInd)\n","\n","    root = train(train_data, Features, gini)\n","\n","    prediction = classify(root, test_data)\n","\n","    actual = getActualLabels(test_data)\n","    predicted = prediction\n","\n","#     print(classification_report(actual, predicted))\n","    precision.append(precision_score(actual, predicted, average='macro'))\n","    recall.append(recall_score(actual, predicted, average='macro'))\n","    f_score.append(f1_score(actual, predicted, average='macro'))\n","\n","    print(\"Training ...\")\n","\n","print(\"Precision Score: \" + str(mean(precision)))\n","print(\"Recall Score: \" + str(mean(recall)))\n","print(\"F-Score: \" + str(mean(f_score)))\n","\n","\n","# ## Part 2\n","# - All\n","# - Unigram, Bigram, Trigram, POS\n","# - Unigram, Bigram, Trigram\n","\n","classes = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n","\n","def getReport(traindata, testdata, uniFlag=True, biFlag=True, triFlag=True, posFlag=True, lenFlag=True, func=gini):\n","    allFeatures = []\n","\n","    if uniFlag:\n","        for y in unigram_counts:\n","            allFeatures.append(Feature(3, y[0]))\n","\n","    if biFlag:\n","        for y in bigram_counts:\n","            allFeatures.append(Feature(4, y[0]))\n","\n","    if triFlag:\n","        for y in trigram_counts:\n","            allFeatures.append(Feature(5, y[0]))\n","\n","    if posFlag:\n","        for y in pos_counts:\n","            allFeatures.append(Feature(6, y[0]))\n","\n","    if lenFlag:\n","        allFeatures.append(Feature(2, avgLength))\n","\n","    print(\"No of Features: \" + str(len(allFeatures)))\n","    print(\"Training ...\")\n","    root = train(traindata, allFeatures, func)\n","    print(\"Predicting ...\")\n","    prediction = classify(root, testdata)\n","    actual = getActualLabels(testdata)\n","    print(\"Prediction done ...\")\n","    # matrix = confusion_matrix(actual, prediction)\n","    # # if (matrix.sum(axis=1)):\n","    # acc = matrix.diagonal()/matrix.sum(axis=1)\n","    matrix = confusion_matrix(actual, prediction)\n","    diagonal = matrix.diagonal().astype(float)\n","    sum_axis_1 = matrix.sum(axis=1, dtype=float)\n","    acc = np.divide(diagonal, sum_axis_1, out=np.zeros_like(diagonal), where=sum_axis_1 != 0)\n","    accuracy_report = dict(zip(classes, acc))\n","\n","    return accuracy_report, root, prediction, actual\n","\n","testdata = build_data(test_url)[0]\n","len(testdata)\n","\n","print(getReport(traindata=data, testdata=testdata)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, func=entropy)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, func=misclassifcation_error)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False, func=entropy)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False, func=misclassifcation_error)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False, posFlag=False)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False, posFlag=False, func=entropy)[0])\n","\n","print(getReport(traindata=data, testdata=testdata, lenFlag=False, posFlag=False, func=misclassifcation_error)[0])\n","print(\"sdfdsfdsf\")\n","\n","#Error Analysis\n","def getWrongPrediction(prediction, actual, dataset):\n","    data_list = []\n","\n","    for i in range(len(prediction)):\n","        if prediction[i] !=actual[i] :\n","            data_list.append(dataset[i])\n","    return data_list\n","\n","_ , root_gini, prediction_gini, actual_gini  = getReport(traindata=data, testdata=testdata)\n","wrong_data = getWrongPrediction(prediction_gini, actual_gini, testdata)\n","print( \" root_gini, prediction_gini, actual_gini\", prediction_gini, actual_gini)\n","len(wrong_data)\n","\n","_ , root_entropy, prediction_entropy, actual_entropy  = getReport(traindata=data, testdata=wrong_data, func=entropy)\n","wrong_data_en = getWrongPrediction(prediction_entropy, actual_entropy, wrong_data)\n","len(wrong_data_en)\n","print(\"root_entropy, prediction_entropy, actual_entropy\", prediction_entropy, actual_entropy)\n","\n","_ , root_mis, prediction_mis, actual_mis  = getReport(traindata=data, testdata=wrong_data, func=misclassifcation_error)\n","wrong_data_mis = getWrongPrediction(prediction_entropy, actual_entropy, wrong_data)\n","len(wrong_data_mis)\n","print(\" root_mis, prediction_mis, actual_mis\",  prediction_mis, actual_mis)"],"metadata":{"id":"cpZyGUrNconx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: Identify Mismatches\n","mismatches_gini_to_mis = [(gini_pred, mis_pred) for gini_pred, mis_pred in zip(prediction_gini, prediction_mis) if gini_pred != mis_pred]\n","mismatches_entropy_to_mis = [(entropy_pred, mis_pred) for entropy_pred, mis_pred in zip(prediction_entropy, prediction_mis) if entropy_pred != mis_pred]\n","\n","# Step 3: Calculate Correction Percentage\n","correction_percentage_gini_to_mis = len(mismatches_gini_to_mis) / len(prediction_gini)\n","correction_percentage_entropy_to_mis = len(mismatches_entropy_to_mis) / len(prediction_entropy)\n","\n","# Report the results\n","print(f\"Percentage of samples corrected by misclassification error from gini index model: {correction_percentage_gini_to_mis * 100:.2f}%\")\n","print(f\"Percentage of samples corrected by misclassification error from cross-entropy model: {correction_percentage_entropy_to_mis * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8pKm-Im5wE6","executionInfo":{"status":"ok","timestamp":1694933420971,"user_tz":-330,"elapsed":522,"user":{"displayName":"harshit nigam","userId":"10797282395490246172"}},"outputId":"d0fcc965-f544-4cc4-e965-e3493f2e77f1"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of samples corrected by misclassification error from gini index model: 0.40%\n","Percentage of samples corrected by misclassification error from cross-entropy model: 0.00%\n"]}]}]}