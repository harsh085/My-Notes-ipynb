{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### NAME : Harshit Nigam\n","### Roll No.:  2311AI52\n","### Subject : Big DATA ANALYTICS\n","### Assignment 9"],"metadata":{"id":"EDh9_pb-r1MV"}},{"cell_type":"markdown","source":["# Classification On Iris Dataset"],"metadata":{"id":"El8PAo5uGNfS"}},{"cell_type":"code","source":["!pip install pyspark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OkpI6GbKXJX","outputId":"3feb9adb-1b73-4f4c-8485-b6f91556be22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}]},{"cell_type":"markdown","source":["## Classification on Iris Dataset Using LogisticRegression"],"metadata":{"id":"p6cnO1hro_MN"}},{"cell_type":"code","source":["# Step 1: Import necessary libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","\n","# Step 3: Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Iris Classification\") \\\n","    .getOrCreate()\n","\n","# Step : Load the Iris dataset\n","iris_df = spark.read.csv(\"/content/Iris.csv\", header=True, inferSchema=True)\n","\n","# Step 5: Prepare the data and encode the label column\n","label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n","iris_df_indexed = label_indexer.fit(iris_df).transform(iris_df)\n","\n","# Assemble features\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n","    outputCol=\"features\"\n",")\n","iris_df_final = assembler.transform(iris_df_indexed)\n","\n","# Step 6: Split the data\n","train_data, test_data = iris_df_final.randomSplit([0.7, 0.3], seed=42)\n","\n","# Step 7: Train the Multinomial Logistic Regression model\n","lr_classifier = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n","# Step 8: Create ParamGridBuilder\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(lr_classifier.maxIter, [10, 20, 30]) \\\n","    .addGrid(lr_classifier.regParam, [0.1, 0.01, 0.001]) \\\n","    .build()\n","\n","# Step 9: Create CrossValidator\n","cross_val = CrossValidator(estimator=lr_classifier,\n","                           estimatorParamMaps=param_grid,\n","                           evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n","                           numFolds=3)\n","\n","# Step 10: Fit CrossValidator\n","cv_model = cross_val.fit(train_data)\n","\n","# Step 11: Make predictions\n","predictions = cv_model.transform(test_data)\n","\n","# Step 12: Evaluate the model\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Accuracy:\", accuracy)\n","\n","# Step 13: Best Model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"Max Iterations:\", best_model.getMaxIter())\n","print(\"Regularization Parameter:\", best_model.getRegParam())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbW4PbXeKSti","outputId":"6157d9c9-3f79-4c7c-fdfb-a3356d7ffd29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9347826086956522\n","Best model parameters:\n","Max Iterations: 10\n","Regularization Parameter: 0.001\n"]}]},{"cell_type":"markdown","source":["## Classification on Iris Dataset Using Decision Tree Classifier"],"metadata":{"id":"2kPP0t4zpaum"}},{"cell_type":"code","source":["# Step 1: Import necessary libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","\n","# Step 3: Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Iris Classification\") \\\n","    .getOrCreate()\n","\n","# Step : Load the Iris dataset\n","iris_df = spark.read.csv(\"/content/Iris.csv\", header=True, inferSchema=True)\n","\n","# Step 5: Prepare the data and encode the label column\n","label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n","iris_df_indexed = label_indexer.fit(iris_df).transform(iris_df)\n","\n","# Assemble features\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n","    outputCol=\"features\"\n",")\n","iris_df_final = assembler.transform(iris_df_indexed)\n","\n","# Step 6: Split the data\n","train_data, test_data = iris_df_final.randomSplit([0.7, 0.3], seed=42)\n","\n","# Step 7: Train the Decision Tree classifier\n","dt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n","# Step 8: Create ParamGridBuilder\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(dt_classifier.maxDepth, [3, 5, 7]) \\\n","    .build()\n","\n","# Step 9: Create CrossValidator\n","cross_val = CrossValidator(estimator=dt_classifier,\n","                            estimatorParamMaps=param_grid,\n","                            evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n","                            numFolds=3)\n","\n","# Step 10: Fit CrossValidator\n","cv_model = cross_val.fit(train_data)\n","\n","# Step 11: Make predictions\n","predictions = cv_model.transform(test_data)\n","\n","# Step 12: Evaluate the model\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Accuracy:\", accuracy)\n","\n","# Step 13: Best Model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"Max Depth:\", best_model.getOrDefault(\"maxDepth\"))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ly8DCcGnNygR","outputId":"8fd9837a-52d2-4a43-c156-caeee3918310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9347826086956522\n","Best model parameters:\n","Max Depth: 3\n"]}]},{"cell_type":"markdown","source":["## Classification on Iris Dataset Using RandomForestClassifier"],"metadata":{"id":"Waj9Ceubpwgm"}},{"cell_type":"code","source":["# Step 1: Import necessary libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","# # Step 3: Create a SparkSession\n","# spark = SparkSession.builder \\\n","#     .appName(\"Iris Classification\") \\\n","#     .getOrCreate()\n","\n","# Step : Load the Iris dataset\n","iris_df = spark.read.csv(\"/content/Iris.csv\", header=True, inferSchema=True)\n","\n","# Step 5: Prepare the data and encode the label column\n","label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n","iris_df_indexed = label_indexer.fit(iris_df).transform(iris_df)\n","\n","# Assemble features\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n","    outputCol=\"features\"\n",")\n","iris_df_final = assembler.transform(iris_df_indexed)\n","\n","# Step 6: Split the data\n","train_data, test_data = iris_df_final.randomSplit([0.7, 0.3], seed=42)\n","\n","# Step 7: Train the RandomForestClassifier\n","rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50)\n","# Step 8: Create ParamGridBuilder\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(rf_classifier.numTrees, [10, 20, 30]) \\\n","    .addGrid(rf_classifier.maxDepth, [3, 5, 7]) \\\n","    .build()\n","\n","# Step 9: Create CrossValidator\n","cross_val = CrossValidator(estimator=rf_classifier,\n","                            estimatorParamMaps=param_grid,\n","                            evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n","                            numFolds=3)\n","\n","# Step 10: Fit CrossValidator\n","cv_model = cross_val.fit(train_data)\n","\n","# Step 11: Make predictions\n","predictions = cv_model.transform(test_data)\n","\n","# Step 12: Evaluate the model\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Accuracy:\", accuracy)\n","\n","# Step 13: Best Model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"Num Trees:\", best_model.getNumTrees)\n","print(\"Max Depth:\", best_model.getOrDefault(\"maxDepth\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pci5crpXPDir","outputId":"49ec9927-45ed-4b3b-ec18-ba9dc3a69556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9130434782608695\n","Best model parameters:\n","Num Trees: 10\n","Max Depth: 5\n"]}]},{"cell_type":"markdown","source":["## Classification on Iris Dataset Using Gradient Boosting Classifier(GBTClassifier)"],"metadata":{"id":"yrqxosocqBEW"}},{"cell_type":"code","source":["## Gradient Boosting\n","# Step 1: Import necessary libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","# # Step 3: Create a SparkSession\n","# spark = SparkSession.builder \\\n","#     .appName(\"Iris Classification\") \\\n","#     .getOrCreate()\n","\n","# Step : Load the Iris dataset\n","iris_df = spark.read.csv(\"/content/Iris.csv\", header=True, inferSchema=True)\n","iris_df = iris_df.filter(iris_df[\"Species\"]!=\"Iris-virginica\")\n","# Step 5: Prepare the data and encode the label column\n","label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n","iris_df_indexed = label_indexer.fit(iris_df).transform(iris_df)\n","\n","# Assemble features\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n","    outputCol=\"features\"\n",")\n","iris_df_final = assembler.transform(iris_df_indexed)\n","\n","# Step 6: Split the data\n","train_data, test_data = iris_df_final.randomSplit([0.7, 0.3], seed=42)\n","\n","# Step 7: Create a Gradient Boosted Trees classifier\n","gbt_classifier = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n","\n","# Step 8: Define a parameter grid for hyperparameter tuning\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(gbt_classifier.maxDepth, [3, 5, 7]) \\\n","    .addGrid(gbt_classifier.maxIter, [10, 20, 30]) \\\n","    .build()\n","\n","# Step 9: Perform cross-validation\n","cross_val = CrossValidator(estimator=gbt_classifier,\n","                           estimatorParamMaps=param_grid,\n","                           evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n","                           numFolds=3)\n","\n","# Step 10: Fit the cross-validator to the training data\n","cv_model = cross_val.fit(train_data)\n","\n","# Step 11: Make predictions on the test data\n","predictions = cv_model.transform(test_data)\n","\n","# Step 12: Evaluate the model's performance\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Accuracy:\", accuracy)\n","\n","# Step 13: Print the best model parameters\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"Max Depth:\", best_model.getMaxDepth())\n","print(\"Max Iterations:\", best_model.getMaxIter())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_zm-1ktPrIA","outputId":"9773744f-8103-42c7-c56b-2ec2a6105307"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n","Best model parameters:\n","Max Depth: 3\n","Max Iterations: 10\n"]}]},{"cell_type":"markdown","source":["# Regression Problem On California Housing DataSet"],"metadata":{"id":"k2tVToIeUeFD"}},{"cell_type":"markdown","source":["## Regression Model on California Housing DataSet Using Linear Regression"],"metadata":{"id":"1MMfpuDXqbqe"}},{"cell_type":"code","source":["#Using Linear Regression Model\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.sql.functions import col\n","\n","# Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Linear Regression with PySpark\") \\\n","    .getOrCreate()\n","\n","# Load California housing dataset\n","data = spark.read.csv(\"/content/housing.csv\", header=True, inferSchema=True)\n","\n","\n","# Drop rows with null values\n","data = data.dropna()\n","\n","# Perform string indexing for the 'ocean_proximity' column\n","string_indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n","data = string_indexer.fit(data).transform(data)\n","\n","# Perform one-hot encoding for the indexed column\n","encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n","data = encoder.fit(data).transform(data)\n","\n","# Assemble features\n","feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n","                   'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_encoded']\n","assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","data = assembler.transform(data)\n","\n","# Rename target column to 'label'\n","data = data.withColumnRenamed('median_house_value', 'label')\n","\n","# Split the data into train and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Define the linear regression model\n","lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n","\n","# Define grid parameters for cross-validation\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n","    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n","    .build()\n","\n","# Define evaluator\n","evaluator = RegressionEvaluator(metricName=\"rmse\")\n","\n","# Define evaluator for R2\n","evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n","\n","# Define cross-validator\n","crossval = CrossValidator(estimator=lr,\n","                          estimatorParamMaps=param_grid,\n","                          evaluator=evaluator,\n","                          numFolds=5)\n","\n","# Train the model\n","cv_model = crossval.fit(train_data)\n","\n","# Make predictions on the test set\n","predictions = cv_model.transform(test_data)\n","\n","# Evaluate the model\n","rmse = evaluator.evaluate(predictions)\n","print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n","\n","# Evaluate the model - R2\n","r2 = evaluator_r2.evaluate(predictions)\n","print(\"R-squared (R2) on test data:\", r2)\n","\n","# Best model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"RegParam:\", best_model._java_obj.getRegParam())\n","print(\"ElasticNetParam:\", best_model._java_obj.getElasticNetParam())\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSsV62lXX1jS","outputId":"944c8d20-5ff8-4c88-b99a-94a22029468a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Root Mean Squared Error (RMSE) on test data: 68641.31132528615\n","R-squared (R2) on test data: 0.6432667986710991\n","Best model parameters:\n","RegParam: 0.5\n","ElasticNetParam: 0.0\n"]}]},{"cell_type":"markdown","source":["## Regression Model on California Housing DataSet Using Decision Tree Regressor"],"metadata":{"id":"59-IMVFfqykl"}},{"cell_type":"code","source":["# Using Decision Tree Regressor\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.regression import DecisionTreeRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.sql.functions import col\n","\n","# Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Decision Tree Regression with PySpark\") \\\n","    .getOrCreate()\n","\n","# Load California housing dataset\n","data = spark.read.csv(\"/content/housing.csv\", header=True, inferSchema=True)\n","\n","\n","\n","# Drop rows with null values\n","data = data.dropna()\n","\n","# Perform string indexing for the 'ocean_proximity' column\n","string_indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n","data = string_indexer.fit(data).transform(data)\n","\n","# Perform one-hot encoding for the indexed column\n","encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n","data = encoder.fit(data).transform(data)\n","\n","# Assemble features\n","feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n","                   'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_encoded']\n","assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","data = assembler.transform(data)\n","\n","# Rename target column to 'label'\n","data = data.withColumnRenamed('median_house_value', 'label')\n","\n","# Split the data into train and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Define the decision tree regression model\n","dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n","\n","# Define grid parameters for cross-validation\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n","    .addGrid(dt.minInstancesPerNode, [1, 3, 5]) \\\n","    .build()\n","\n","# Define evaluator for RMSE\n","evaluator_rmse = RegressionEvaluator(metricName=\"rmse\")\n","\n","# Define evaluator for R2\n","evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n","\n","# Define cross-validator\n","crossval = CrossValidator(estimator=dt,\n","                          estimatorParamMaps=param_grid,\n","                          evaluator=evaluator_rmse,  # Use RMSE for model selection during cross-validation\n","                          numFolds=5)\n","\n","# Train the model\n","cv_model = crossval.fit(train_data)\n","\n","# Make predictions on the test set\n","predictions = cv_model.transform(test_data)\n","\n","# Evaluate the model - RMSE\n","rmse = evaluator_rmse.evaluate(predictions)\n","print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n","\n","# Evaluate the model - R2\n","r2 = evaluator_r2.evaluate(predictions)\n","print(\"R-squared (R2) on test data:\", r2)\n","\n","# Best model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"MaxDepth:\", best_model._java_obj.getMaxDepth())\n","print(\"MinInstancesPerNode:\", best_model._java_obj.getMinInstancesPerNode())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZU7wNcwasnS","outputId":"37a2ef1b-d146-4713-feee-74bda2087be8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Root Mean Squared Error (RMSE) on test data: 66070.57962413561\n","R-squared (R2) on test data: 0.6694869427206733\n","Best model parameters:\n","MaxDepth: 7\n","MinInstancesPerNode: 5\n"]}]},{"cell_type":"markdown","source":["## Regression Model on California Housing DataSet Using Gradient Boosting Regressor(GBTRegressor)"],"metadata":{"id":"Gpckf06HrA8t"}},{"cell_type":"code","source":["# By using Gradient Boosting Regressor\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.regression import GBTRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.sql.functions import col\n","\n","# Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Gradient Boosting Regression with PySpark\") \\\n","    .getOrCreate()\n","\n","# Load California housing dataset\n","data = spark.read.csv(\"/content/housing.csv\", header=True, inferSchema=True)\n","\n","\n","# Drop rows with null values\n","data = data.dropna()\n","\n","# Perform string indexing for the 'ocean_proximity' column\n","string_indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n","data = string_indexer.fit(data).transform(data)\n","\n","# Perform one-hot encoding for the indexed column\n","encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n","data = encoder.fit(data).transform(data)\n","\n","# Assemble features\n","feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n","                   'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_encoded']\n","assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","data = assembler.transform(data)\n","\n","# Rename target column to 'label'\n","data = data.withColumnRenamed('median_house_value', 'label')\n","\n","# Split the data into train and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Define the Gradient Boosting Regression model\n","gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n","\n","# Define grid parameters for cross-validation\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n","    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n","    .build()\n","\n","# Define evaluator for RMSE\n","evaluator_rmse = RegressionEvaluator(metricName=\"rmse\")\n","\n","# Define evaluator for R2\n","evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n","\n","# Define cross-validator\n","crossval = CrossValidator(estimator=gbt,\n","                          estimatorParamMaps=param_grid,\n","                          evaluator=evaluator_rmse,  # Use RMSE for model selection during cross-validation\n","                          numFolds=5)\n","\n","# Train the model\n","cv_model = crossval.fit(train_data)\n","\n","# Make predictions on the test set\n","predictions = cv_model.transform(test_data)\n","\n","# Evaluate the model - RMSE\n","rmse = evaluator_rmse.evaluate(predictions)\n","print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n","\n","# Evaluate the model - R2\n","r2 = evaluator_r2.evaluate(predictions)\n","print(\"R-squared (R2) on test data:\", r2)\n","\n","# Best model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"MaxDepth:\", best_model._java_obj.getMaxDepth())\n","print(\"MaxIter:\", best_model._java_obj.getMaxIter())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHxkrqDkcGzJ","outputId":"7346fcde-9eb5-43b9-d116-610a6fefd51f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Root Mean Squared Error (RMSE) on test data: 56125.89882890557\n","R-squared (R2) on test data: 0.7614941663434059\n","Best model parameters:\n","MaxDepth: 7\n","MaxIter: 30\n"]}]},{"cell_type":"markdown","source":["## Regression Model on California Housing DataSet Using RandomForestRegressor"],"metadata":{"id":"4XyoRVO5rh8V"}},{"cell_type":"code","source":["# By Using RandomForestRegressor\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.sql.functions import col\n","\n","# Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Random Forest Regression with PySpark\") \\\n","    .getOrCreate()\n","\n","# Load California housing dataset\n","data = spark.read.csv(\"/content/housing.csv\", header=True, inferSchema=True)\n","\n","# # Check for null values in the dataset\n","# null_counts = [(col_name, data.where(col(col_name).isNull()).count()) for col_name in data.columns]\n","# print(\"Null value counts:\")\n","# for col_name, count in null_counts:\n","#     print(col_name + \": \" + str(count))\n","\n","# Drop rows with null values\n","data = data.dropna()\n","\n","# Perform string indexing for the 'ocean_proximity' column\n","string_indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n","data = string_indexer.fit(data).transform(data)\n","\n","# Perform one-hot encoding for the indexed column\n","encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n","data = encoder.fit(data).transform(data)\n","\n","# Assemble features\n","feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n","                   'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_encoded']\n","assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","data = assembler.transform(data)\n","\n","# Rename target column to 'label'\n","data = data.withColumnRenamed('median_house_value', 'label')\n","\n","# Split the data into train and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Define the Random Forest Regression model\n","rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n","\n","# Define grid parameters for cross-validation\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(rf.numTrees, [10, 20, 30]) \\\n","    .addGrid(rf.maxDepth, [3, 5, 7]) \\\n","    .build()\n","\n","# Define evaluator for RMSE\n","evaluator_rmse = RegressionEvaluator(metricName=\"rmse\")\n","\n","# Define evaluator for R2\n","evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n","\n","# Define cross-validator\n","crossval = CrossValidator(estimator=rf,\n","                          estimatorParamMaps=param_grid,\n","                          evaluator=evaluator_rmse,  # Use RMSE for model selection during cross-validation\n","                          numFolds=5)\n","\n","# Train the model\n","cv_model = crossval.fit(train_data)\n","\n","# Make predictions on the test set\n","predictions = cv_model.transform(test_data)\n","\n","# Evaluate the model - RMSE\n","rmse = evaluator_rmse.evaluate(predictions)\n","print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n","\n","# Evaluate the model - R2\n","r2 = evaluator_r2.evaluate(predictions)\n","print(\"R-squared (R2) on test data:\", r2)\n","\n","# Best model\n","best_model = cv_model.bestModel\n","print(\"Best model parameters:\")\n","print(\"NumTrees:\", best_model._java_obj.getNumTrees())\n","print(\"MaxDepth:\", best_model._java_obj.getMaxDepth())\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KcP2tY9cVOp","outputId":"620092c3-3f46-4f79-da56-e899849052fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Root Mean Squared Error (RMSE) on test data: 62810.00160021876\n","R-squared (R2) on test data: 0.7013035942163848\n","Best model parameters:\n","NumTrees: 30\n","MaxDepth: 7\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lnTQkGoTf_14"},"execution_count":null,"outputs":[]}]}